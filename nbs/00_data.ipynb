{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from pathlib import Path\n",
    "import urllib.request as u_request\n",
    "from zipfile import ZipFile\n",
    "import csv \n",
    "import pandas as pd\n",
    "\n",
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "> Here we deal with the data acquisition and processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data acquirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def acquire_data(train=True, val=True):\n",
    "    \"\"\"Obtains the train and validation datasets of the competition. \n",
    "    The train url maight fail. Get it from https://drive.google.com/drive/folders/1RXziMCO4Y0Fmpm5bmjcpy-Genhzv4QJ4\"\"\"\n",
    "    data_path = Path(\"../data\")\n",
    "    data_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    train_url = (\"https://doc-4k-88-drive-data-export.googleusercontent.com/download/qh9kfuk2n3khcj0qvrn9t3a4j19nve1a/\" + \n",
    "                \"rqpd3tajosn0gta5f9mmbbb1e4u8csnn/1599642000000/17390da5-4567-4189-8a62-1749e1b19b06/108540842544374891611/\" + \n",
    "                \"ADt3v-N9HwRAxXINIFMKGcsrjzMlrvhOOYitRyphFom1Ma-CUUekLTkDp75fOegXlyeVVrTPjlnqDaK0g6iI7eDL9YJw91-\" + \n",
    "                \"jiityR3iTfrysZP6hpGA62c4lkZbjGp_NJL-XSDUlPcwiVi5Hd5rFtH1YYP0tiiFCoJZsTT4akE8fjdrkZU7vaqFznxuyQDA8YGaiuYlKu\" + \n",
    "                \"-F1HiAc9kG_k9EMgkMncNflNJtlugxH5pFcNDdrYiOzIINRIRivt5ScquQ_s4KyuV-zYOQ_g2_VYri8YAg0IqbBrcO-exlp5j-\" +\n",
    "                \"t02GDh5JZKU3Hky5b70Z8brCL5lvK0SFAFIKOer45ZrFaACA3HGRNJg==?authuser=0&nonce=k5g7m53pp3cqq&user=\" + \n",
    "                \"108540842544374891611&hash=m7kmrh87gmekjhrdcpbhuf1kj13ui0l2\")\n",
    "    val_url = (\"https://competitions.codalab.org/my/datasets/download/e2cb70bd-d404-467a-a950-78d12f51e0f6\")\n",
    "\n",
    "    if train: \n",
    "        data = _download_bytes(train_url)\n",
    "        _write_bytes(data, data_path)\n",
    "        train_path = data_path/\"Development dataset for Training\"\n",
    "        train_path.rename(train_path.parent/\"train\")\n",
    "        \n",
    "    if val: \n",
    "        data = _download_bytes(val_url)\n",
    "        _write_bytes(data, data_path)\n",
    "        val_path = data_path/\"validation_for_scoring\"\n",
    "        val_path.rename(val_path.parent/\"val\")\n",
    "        \n",
    "    rmtree(data_path/\"__MACOSX\")\n",
    "    \n",
    "def _download_bytes(url):\n",
    "    \"Downloads data from `url` as bytes\"\n",
    "    u = u_request.urlopen(url)\n",
    "    data = u.read()\n",
    "    u.close()\n",
    "    return data\n",
    "\n",
    "def _write_bytes(data, path):\n",
    "    \"Saves `data` (bytes) into path.\"\n",
    "    zip_path = _zip_bytes(data)\n",
    "    _unzip_file(zip_path, new_path=path)\n",
    "\n",
    "def _zip_bytes(data, path=None):\n",
    "    \"Saves bytes data as .zip in `path`.\"\n",
    "    if path is None: path = Path(\"../temp\")\n",
    "    zip_path = path.with_suffix(\".zip\")\n",
    "    with open(zip_path, \"wb\") as f:\n",
    "        f.write(data)\n",
    "    return zip_path\n",
    "        \n",
    "def _unzip_file(file_path, new_path=None, purge=True):\n",
    "    \"Unzips file in `file_path` to `new_path`.\"\n",
    "    if new_path is None: new_path = file_path.with_suffix(\"\")\n",
    "    zip_path = file_path.with_suffix(\".zip\")\n",
    "    with ZipFile(zip_path, 'r') as f:\n",
    "        f.extractall(new_path)\n",
    "    if purge: zip_path.unlink()\n",
    "        \n",
    "def rmtree(root):\n",
    "    for p in root.iterdir():\n",
    "        if p.is_dir(): rmtree(p)\n",
    "        else: p.unlink()\n",
    "    root.rmdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_data(task, dim=1, train=True):\n",
    "    \"Loads train or val data of corresponding dimension.\"\n",
    "    path = Path(\"../data/train\") if train else Path(\"../data/val\")\n",
    "    try: \n",
    "        df = pd.read_pickle(path/f\"task{task}.pkl\")\n",
    "    except: \n",
    "        _txt2df(task, train=train, val=not train)\n",
    "        df = pd.read_pickle(path/f\"task{task}.pkl\")    \n",
    "    return df[df['dim']==dim].reset_index()\n",
    "\n",
    "def _txt2df(task, train=True, val=False):\n",
    "    \"Extracts dataset and saves it in df form\"\n",
    "    if train:\n",
    "        df = pd.DataFrame(columns=['dim', 'y', 'x', 'len'], dtype=object)\n",
    "        train_path = Path(\"../data/train\")\n",
    "        if not (train_path/f\"task{task}.txt\").exists(): acquire_data(train=True, val=False)\n",
    "        with open(train_path/f\"task{task}.txt\", \"r\") as D, open(train_path/f\"ref{task}.txt\") as Y:\n",
    "            trajs = csv.reader(D, delimiter=\";\", lineterminator=\"\\n\", quoting=csv.QUOTE_NONNUMERIC)\n",
    "            labels = csv.reader(Y, delimiter=\";\", lineterminator=\"\\n\", quoting=csv.QUOTE_NONNUMERIC)\n",
    "            for t, y in zip(trajs, labels):\n",
    "                dim, x = int(t[0]), t[1:]\n",
    "                x = tensor(x).view(dim, -1).T\n",
    "                df = df.append({'dim': dim, 'y': y[1], 'x': x, 'len': len(x)}, ignore_index=True)\n",
    "\n",
    "        df.to_pickle(train_path/f\"task{task}.pkl\")\n",
    "        \n",
    "    if val: \n",
    "        df = pd.DataFrame(columns=['dim', 'x', 'len'], dtype=object)\n",
    "        val_path = Path(\"../data/val\")\n",
    "        task_path = val_path/f\"task{task}.txt\"\n",
    "        if not task_path.exists(): acquire_data(train=False, val=True)\n",
    "        with open(task_path, \"r\") as D:\n",
    "            trajs = csv.reader(D, delimiter=\";\", lineterminator=\"\\n\", quoting=csv.QUOTE_NONNUMERIC)\n",
    "            for t in trajs:\n",
    "                dim, x = int(t[0]), t[1:]\n",
    "                x = tensor(x).view(dim, -1).T\n",
    "                df = df.append({'dim': dim, 'x': x, 'len': len(x)}, ignore_index=True)\n",
    "        \n",
    "        df['y'] = \"\"\n",
    "        df.to_pickle(val_path/f\"task{task}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_discriminative_dls(task, dim=1, bs=64, split_pct=0.2, train=True, **kwargs):\n",
    "    \"Obtain `DataLoaders` for classification/regression models.\"\n",
    "    data = load_data(task, dim=dim, train=train)\n",
    "    ds = L(zip(data['x'], data['y']))\n",
    "    idx = L(int(i) for i in torch.randperm(data.shape[0]))\n",
    "    cut = int(data.shape[0]*split_pct)\n",
    "    \n",
    "    train_ds, val_ds = ds[idx[cut:]], ds[idx[:cut]]\n",
    "    sorted_dl = partial(SortedDL, before_batch=partial(pad_trajectories, **kwargs), shuffle=True)\n",
    "    dls = DataLoaders.from_dsets(train_ds, val_ds, bs=bs, dl_type=sorted_dl, device=default_device())\n",
    "    return dls\n",
    "\n",
    "def get_validation_dl(task, dim=1, bs=64, **kwargs):\n",
    "    \"Obtain `DataLoaders` for validation.\"\n",
    "    data = load_data(task, dim=dim, train=False)\n",
    "    ds = L(zip(data['x'], data['y']))\n",
    "    return DataLoader(ds, bs=bs, before_batch=partial(pad_trajectories, **kwargs), device=default_device())\n",
    "\n",
    "def pad_trajectories(samples, pad_value=0, pad_first=True, backwards=False):\n",
    "    \"Pads trajectories assuming shape (len, dim)\"\n",
    "    max_len = max([s.shape[0] for s, _ in samples])\n",
    "    if backwards: pad_first = not pad_first\n",
    "    def _pad_sample(s):\n",
    "        diff = max_len - s.shape[0]\n",
    "        pad = s.new_zeros((diff, s.shape[1])) + pad_value\n",
    "        pad_s = torch.cat([pad, s] if pad_first else [s, pad])\n",
    "        if backwards: pad_s = pad_s.flip(0)\n",
    "        return pad_s\n",
    "    return L((_pad_sample(s), y) for s, y in samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = get_discriminative_dls(task=1, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 999, 2]), torch.Size([64]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = dls.one_batch()\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_task(models, task, dims, bs=256):\n",
    "    \"Validates `models` on task for `dims`.\"\n",
    "    if not hasattr(models, '__iter__'): models = [models]\n",
    "    if not hasattr(dims, '__iter__'): dims = [dims]\n",
    "    if len(models) != len(dims): \n",
    "        raise InputError(f\"There are {len(models)} models and {len(dims)} dimensions\")\n",
    "    pred_path = Path(\"../data/preds\")\n",
    "    pred_path.mkdir(exist_ok=True)\n",
    "    task_path = pred_path/f\"task{task}.txt\"\n",
    "    preds_dim = []\n",
    "    for model, dim in zip(models, dims): preds_dim.append(validate_model(model, task, dim=dim, bs=bs))\n",
    "    \n",
    "    with open(task_path, \"w\") as f:\n",
    "        for dim, preds in zip(dims, preds_dim):\n",
    "            for pred in preds:\n",
    "                f.write(f\"{int(dim)}; {str(pred.item())}\\n\")\n",
    "    \n",
    "    \n",
    "def validate_model(model, task, dim=1, bs=256):\n",
    "    \"Validates model on specific task and dimension.\"\n",
    "    val_dl = get_validation_dl(task, dim=dim, bs=bs)\n",
    "    return torch.cat([model(batch).detach().cpu() for batch, _ in val_dl])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_data.ipynb.\n",
      "Converted 01_models.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
