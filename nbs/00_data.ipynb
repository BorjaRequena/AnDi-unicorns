{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from pathlib import Path\n",
    "import urllib.request as u_request\n",
    "from zipfile import ZipFile\n",
    "import csv \n",
    "import pandas as pd\n",
    "from andi import andi_datasets, normalize\n",
    "import numpy as np\n",
    "\n",
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "> Here we deal with the data acquisition and processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data acquirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "DATA_PATH = Path(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def acquire_data(train=True, val=True):\n",
    "    \"\"\"Obtains the train and validation datasets of the competition. \n",
    "    The train url maight fail. Get it from https://drive.google.com/drive/folders/1RXziMCO4Y0Fmpm5bmjcpy-Genhzv4QJ4\"\"\"\n",
    "    DATA_PATH.mkdir(exist_ok=True)\n",
    "    \n",
    "    train_url = (\"https://doc-4k-88-drive-data-export.googleusercontent.com/download/qh9kfuk2n3khcj0qvrn9t3a4j19nve1a/\" + \n",
    "                \"rqpd3tajosn0gta5f9mmbbb1e4u8csnn/1599642000000/17390da5-4567-4189-8a62-1749e1b19b06/108540842544374891611/\" + \n",
    "                \"ADt3v-N9HwRAxXINIFMKGcsrjzMlrvhOOYitRyphFom1Ma-CUUekLTkDp75fOegXlyeVVrTPjlnqDaK0g6iI7eDL9YJw91-\" + \n",
    "                \"jiityR3iTfrysZP6hpGA62c4lkZbjGp_NJL-XSDUlPcwiVi5Hd5rFtH1YYP0tiiFCoJZsTT4akE8fjdrkZU7vaqFznxuyQDA8YGaiuYlKu\" + \n",
    "                \"-F1HiAc9kG_k9EMgkMncNflNJtlugxH5pFcNDdrYiOzIINRIRivt5ScquQ_s4KyuV-zYOQ_g2_VYri8YAg0IqbBrcO-exlp5j-\" +\n",
    "                \"t02GDh5JZKU3Hky5b70Z8brCL5lvK0SFAFIKOer45ZrFaACA3HGRNJg==?authuser=0&nonce=k5g7m53pp3cqq&user=\" + \n",
    "                \"108540842544374891611&hash=m7kmrh87gmekjhrdcpbhuf1kj13ui0l2\")\n",
    "    val_url = (\"https://competitions.codalab.org/my/datasets/download/7ea12913-dfcf-4a50-9f5d-8bf9666e9bb4\")\n",
    "\n",
    "    if train: \n",
    "        data = _download_bytes(train_url)\n",
    "        _write_bytes(data, DATA_PATH)\n",
    "        train_path = DATA_PATH/\"Development dataset for Training\"\n",
    "        train_path.rename(train_path.parent/\"train\")\n",
    "        \n",
    "    if val: \n",
    "        data = _download_bytes(val_url)\n",
    "        _write_bytes(data, DATA_PATH)\n",
    "        val_path = DATA_PATH/\"validation_for_scoring\"\n",
    "        val_path.rename(val_path.parent/\"val\")\n",
    "        \n",
    "    rmtree(DATA_PATH/\"__MACOSX\")\n",
    "    \n",
    "def _download_bytes(url):\n",
    "    \"Downloads data from `url` as bytes\"\n",
    "    u = u_request.urlopen(url)\n",
    "    data = u.read()\n",
    "    u.close()\n",
    "    return data\n",
    "\n",
    "def _write_bytes(data, path):\n",
    "    \"Saves `data` (bytes) into path.\"\n",
    "    zip_path = _zip_bytes(data)\n",
    "    _unzip_file(zip_path, new_path=path)\n",
    "\n",
    "def _zip_bytes(data, path=None):\n",
    "    \"Saves bytes data as .zip in `path`.\"\n",
    "    if path is None: path = Path(\"../temp\")\n",
    "    zip_path = path.with_suffix(\".zip\")\n",
    "    with open(zip_path, \"wb\") as f:\n",
    "        f.write(data)\n",
    "    return zip_path\n",
    "        \n",
    "def _unzip_file(file_path, new_path=None, purge=True):\n",
    "    \"Unzips file in `file_path` to `new_path`.\"\n",
    "    if new_path is None: new_path = file_path.with_suffix(\"\")\n",
    "    zip_path = file_path.with_suffix(\".zip\")\n",
    "    with ZipFile(zip_path, 'r') as f:\n",
    "        f.extractall(new_path)\n",
    "    if purge: zip_path.unlink()\n",
    "        \n",
    "def rmtree(root):\n",
    "    for p in root.iterdir():\n",
    "        if p.is_dir(): rmtree(p)\n",
    "        else: p.unlink()\n",
    "    root.rmdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['dim', 'model', 'exp', 'x', 'len'], dtype=object)\n",
    "for dim in range(1, 4):\n",
    "    trajs = pd.read_pickle(DATA_PATH/f\"custom_val/dataset_{dim}D_task_2.pkl\")['dataset_og_t2']\n",
    "    for traj in trajs:\n",
    "        model, exp, x = traj[0], traj[1], traj[2:]  \n",
    "        x = tensor(x).view(dim,-1).T\n",
    "        x = x[:torch.randint(10, 1000, (1,))]\n",
    "        df = df.append({'dim': dim, 'model': model, 'exp': exp, 'x': x, 'len': len(x)}, ignore_index=True)\n",
    "        \n",
    "    df.to_pickle(DATA_PATH/f\"custom_val/custom_{dim}D.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_custom_data(dim=1, models=None, exps=None, path=None):\n",
    "    \"Loads data from custom dataset.\"\n",
    "    path = DATA_PATH/f\"custom{dim}.pkl\" if path is None else path\n",
    "    df = pd.read_pickle(path)\n",
    "    mod_mask = sum([df['model'] == m for m in models]) if models is not None else np.ones(df.shape[0], dtype=bool)\n",
    "    exp_mask = sum([df['exp'] == e for e in exps]) if exps is not None else np.ones(df.shape[0], dtype=bool)\n",
    "    mask = mod_mask & exp_mask    \n",
    "    return df[mask].reset_index(drop=True)\n",
    "    \n",
    "def load_data(task, dim=1, ds='train'):\n",
    "    \"Loads 'train' or 'val' data of corresponding dimension.\"\n",
    "    path = DATA_PATH/ds\n",
    "    try: \n",
    "        df = pd.read_pickle(path/f\"task{task}.pkl\")\n",
    "    except: \n",
    "        _txt2df(task, ds=[ds])\n",
    "        df = pd.read_pickle(path/f\"task{task}.pkl\")    \n",
    "    return df[df['dim']==dim].reset_index(drop=True)\n",
    "\n",
    "def _txt2df(task, ds=['train', 'val']):\n",
    "    \"Extracts dataset and saves it in df form\"\n",
    "    if 'train' in ds:\n",
    "        df = pd.DataFrame(columns=['dim', 'y', 'x', 'len'], dtype=object)\n",
    "        train_path = DATA_PATH/\"train\"\n",
    "        if not (train_path/f\"task{task}.txt\").exists(): acquire_data(train=True, val=False)\n",
    "        with open(train_path/f\"task{task}.txt\", \"r\") as D, open(train_path/f\"ref{task}.txt\") as Y:\n",
    "            trajs = csv.reader(D, delimiter=\";\", lineterminator=\"\\n\", quoting=csv.QUOTE_NONNUMERIC)\n",
    "            labels = csv.reader(Y, delimiter=\";\", lineterminator=\"\\n\", quoting=csv.QUOTE_NONNUMERIC)\n",
    "            for t, y in zip(trajs, labels):\n",
    "                dim, x = int(t[0]), t[1:]\n",
    "                x = tensor(x).view(dim, -1).T\n",
    "                label = tensor(y[1:]) if task is 3 else y[1]\n",
    "                df = df.append({'dim': dim, 'y': label, 'x': x, 'len': len(x)}, ignore_index=True)\n",
    "\n",
    "        df.to_pickle(train_path/f\"task{task}.pkl\")\n",
    "        \n",
    "    if 'val' in ds: \n",
    "        df = pd.DataFrame(columns=['dim', 'x', 'len'], dtype=object)\n",
    "        val_path = DATA_PATH/\"val\"\n",
    "        task_path = val_path/f\"task{task}.txt\"\n",
    "        if not task_path.exists(): acquire_data(train=False, val=True)\n",
    "        with open(task_path, \"r\") as D:\n",
    "            trajs = csv.reader(D, delimiter=\";\", lineterminator=\"\\n\", quoting=csv.QUOTE_NONNUMERIC)\n",
    "            for t in trajs:\n",
    "                dim, x = int(t[0]), t[1:]\n",
    "                x = tensor(x).view(dim, -1).T\n",
    "                df = df.append({'dim': dim, 'x': x, 'len': len(x)}, ignore_index=True)\n",
    "        \n",
    "        df['y'] = \"\"\n",
    "        df.to_pickle(val_path/f\"task{task}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pad_trajectories(samples, pad_value=0, pad_first=True, backwards=False):\n",
    "    \"Pads trajectories assuming shape (len, dim)\"\n",
    "    max_len = max([s.shape[0] for s, _ in samples])\n",
    "    if backwards: pad_first = not pad_first\n",
    "    def _pad_sample(s):\n",
    "        s = normalize_trajectory(s)\n",
    "        diff = max_len - s.shape[0]\n",
    "        pad = s.new_zeros((diff, s.shape[1])) + pad_value\n",
    "        pad_s = torch.cat([pad, s] if pad_first else [s, pad])\n",
    "        if backwards: pad_s = pad_s.flip(0)\n",
    "        return pad_s\n",
    "    return L((_pad_sample(s), y) for s, y in samples)\n",
    "\n",
    "def normalize_trajectory(traj):\n",
    "    \"Normalizes the trajectory displacements.\"\n",
    "    n_traj = torch.zeros_like(traj)\n",
    "    disp = traj[1:]-traj[:-1]\n",
    "    n_traj[1:] = disp.div_(disp.std(0)).cumsum(0)\n",
    "    return n_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(pad_trajectories)\n",
    "def get_custom_dls(target='model', dim=1, models=None, exps=None, bs=128, split_pct=0.2, path=None, balance=False, **kwargs):\n",
    "    \"Obtain `DataLoaders` from custom dataset filtered by `models` and `exps` to predict `target`.\"\n",
    "    data = load_custom_data(dim=dim, models=models, exps=exps, path=path)\n",
    "    if balance: data = _subsample_df(data)\n",
    "    ds = L(zip(data['x'], data[target])) if target is 'exp' else L(zip(data['x'], data[target].astype(int)))\n",
    "    sorted_dl = partial(SortedDL, before_batch=partial(pad_trajectories, **kwargs), shuffle=True)\n",
    "    return get_dls_from_ds(ds, sorted_dl, split_pct=split_pct, bs=bs)\n",
    "    \n",
    "@delegates(pad_trajectories)\n",
    "def get_discriminative_dls(task, dim=1, bs=128, split_pct=0.2, ds='train', **kwargs):\n",
    "    \"Obtain `DataLoaders` for classification/regression models.\"\n",
    "    data = load_data(task, dim=dim, ds=ds)\n",
    "    ds = L(zip(data['x'], data['y'])) if task==1 else L(zip(data['x'], data['y'].astype(int)))\n",
    "    sorted_dl = partial(SortedDL, before_batch=partial(pad_trajectories, **kwargs), shuffle=True)\n",
    "    return get_dls_from_ds(ds, sorted_dl, split_pct=split_pct, bs=bs)\n",
    "\n",
    "@delegates(SortedDL.__init__)\n",
    "def get_turning_point_dls(task=3, dim=1, bs=128, split_pct=0.2, ds='train', **kwargs):\n",
    "    \"Obtain `DataLoaders` to predict change points in trajecotries.\"\n",
    "    data = load_data(task, dim=dim, ds=ds)\n",
    "    ds = L(zip(data['x'], torch.stack(list(data['y'].values))[:, 0]))\n",
    "    sorted_dl = partial(SortedDL, shuffle=True, **kwargs)\n",
    "    return get_dls_from_ds(ds, sorted_dl, split_pct=split_pct, bs=bs)\n",
    "    \n",
    "@delegates(pad_trajectories)\n",
    "def get_1vall_dls(target=0, dim=1, models=None, exps=None, bs=128, split_pct=0.2, **kwargs):\n",
    "    data = load_custom_data(dim=dim, models=models, exps=exps)\n",
    "    x, y = data['x'], (data['model'] != target).astype(int)\n",
    "    ds = L(zip(x, y)) \n",
    "    sorted_dl = partial(SortedDL, before_batch=partial(pad_trajectories, **kwargs), shuffle=True)\n",
    "    return get_dls_from_ds(ds, sorted_dl, split_pct=split_pct, bs=bs)\n",
    "    \n",
    "@delegates(pad_trajectories)\n",
    "def get_validation_dl(task, dim=1, bs=64, ds='val', **kwargs):\n",
    "    \"Obtain `DataLoaders` for validation.\"\n",
    "    data = load_data(task, dim=dim, ds=ds)\n",
    "    ds = L(zip(data['x'], data['y']))\n",
    "    return DataLoader(ds, bs=bs, before_batch=partial(pad_trajectories, **kwargs), device=default_device())\n",
    "\n",
    "def get_dls_from_ds(ds, dl_type, split_pct=0.2, bs=128):\n",
    "    idx = L(int(i) for i in torch.randperm(len(ds)))\n",
    "    cut = int(len(ds)*split_pct)\n",
    "    \n",
    "    train_ds, val_ds = ds[idx[cut:]], ds[idx[:cut]]\n",
    "    return DataLoaders.from_dsets(train_ds, val_ds, bs=bs, dl_type=dl_type, device=default_device())\n",
    "\n",
    "def _subsample_df(df):\n",
    "    \"Subsamples df to balance models\"\n",
    "    models = df.model.unique()\n",
    "    max_s = min([len(df[df.model==m]) for m in models])\n",
    "    sub_dfs = [df[df.model==m].sample(frac=1)[:max_s] for m in models]\n",
    "    return pd.concat(sub_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = get_discriminative_dls(task=1, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 999, 2]), torch.Size([128]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = dls.one_batch()\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_custom_dataset(N, max_T=1000, min_T=10, dimensions=[1, 2, 3], save=True):\n",
    "    ad = andi_datasets()\n",
    "    exponents = np.arange(0.05, 2.01, 0.05)\n",
    "    n_exp, n_models = len(exponents), len(ad.avail_models_name)\n",
    "    # Trajectories per model and exponent. Arbitrarely chose to fulfill balanced classes\n",
    "    N_per_model = np.ceil(1.6*N/5)\n",
    "    subdif, superdif = n_exp//2, n_exp//2+1\n",
    "    num_per_class = np.zeros((n_models, n_exp))\n",
    "    num_per_class[:2,:subdif] = np.ceil(N_per_model/subdif)         # ctrw, attm\n",
    "    num_per_class[2, :] = np.ceil(N_per_model/(n_exp-1))            # fbm\n",
    "    num_per_class[2, exponents == 2] = 0                            # fbm can't be ballistic\n",
    "    num_per_class[3, subdif:] = np.ceil((N_per_model/superdif)*0.8) # lw\n",
    "    num_per_class[4, :] = np.ceil(N_per_model/n_exp)                # sbm\n",
    "    \n",
    "    for dim in dimensions:             \n",
    "        dataset = ad.create_dataset(T=max_T, N=num_per_class, exponents=exponents, \n",
    "                                    dimension=dim, models=np.arange(n_models))            \n",
    "\n",
    "        # Normalize trajectories\n",
    "        n_traj = dataset.shape[0]\n",
    "        norm_trajs = normalize(dataset[:, 2:].reshape(n_traj*dim, max_T))\n",
    "        dataset[:, 2:] = norm_trajs.reshape(dataset[:, 2:].shape)\n",
    "\n",
    "        # Add localization error, Gaussian noise with sigma = [0.1, 0.5, 1]\n",
    "        loc_error_amplitude = np.random.choice(np.array([0.1, 0.5, 1]), size=n_traj*dim)\n",
    "        loc_error = (np.random.randn(n_traj*dim, int(max_T)).transpose()*loc_error_amplitude).transpose()\n",
    "        dataset = ad.create_noisy_localization_dataset(dataset, dimension=dim, T=max_T, noise_func=loc_error)\n",
    "        \n",
    "        # Add random diffusion coefficients\n",
    "        trajs = dataset[:, 2:].reshape(n_traj*dim, max_T)\n",
    "        displacements = trajs[:, 1:] - trajs[:, :-1]\n",
    "        # Get new diffusion coefficients and displacements\n",
    "        diffusion_coefficients = np.random.randn(trajs.shape[0])\n",
    "        new_displacements = (displacements.transpose()*diffusion_coefficients).transpose()  \n",
    "        # Generate new trajectories and add to dataset\n",
    "        new_trajs = np.cumsum(new_displacements, axis=1)\n",
    "        new_trajs = np.concatenate((np.zeros((new_trajs.shape[0], 1)), new_trajs), axis=1)\n",
    "        dataset[:, 2:] = new_trajs.reshape(dataset[:, 2:].shape)\n",
    "        \n",
    "        df = pd.DataFrame(columns=['dim', 'model', 'exp', 'x', 'len'], dtype=object)\n",
    "        for traj in dataset:\n",
    "            mod, exp, x = int(traj[0]), traj[1], traj[2:]\n",
    "            x = cut_trajectory(x, np.random.randint(min_T, max_T), dim=dim)\n",
    "            x = tensor(x).view(dim, -1).T\n",
    "            df = df.append({'dim': dim, 'model': mod, 'exp': exp, 'x': x, 'len': len(x)}, ignore_index=True)\n",
    "            \n",
    "        if save:\n",
    "            DATA_PATH.mkdir(exist_ok=True)\n",
    "            ds_path = DATA_PATH/f\"custom{dim}.pkl\"\n",
    "            df.to_pickle(ds_path, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def cut_trajectory(traj, t_cut, dim=1):\n",
    "    \"Takes a trajectory and cuts it to `T_max` length.\"\n",
    "    cut_traj = traj.reshape(dim, -1)[:, :t_cut]\n",
    "    return cut_traj.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brequena/anaconda3/lib/python3.7/site-packages/andi/diffusion_models.py:189: RuntimeWarning: overflow encountered in power\n",
      "  dt = (1-np.random.rand(T))**(-1/sigma)\n",
      "/home/brequena/anaconda3/lib/python3.7/site-packages/andi/diffusion_models.py:308: RuntimeWarning: overflow encountered in power\n",
      "  dt = (1-np.random.rand(T))**(-1/sigma)\n"
     ]
    }
   ],
   "source": [
    "df = create_custom_dataset(20, max_T=25, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def validate_model(model, task, dim=1, bs=256, act=False, **kwargs):\n",
    "    \"Validates model on specific task and dimension.\"\n",
    "    val_dl = get_validation_dl(task, dim=dim, bs=bs, **kwargs)\n",
    "    if act: return torch.cat([to_detach(model(batch)[0].softmax(1)) for batch, _ in val_dl]) \n",
    "    else:   return torch.cat([to_detach(model(batch)) for batch, _ in val_dl])    \n",
    "    \n",
    "@delegates(validate_model)\n",
    "def validate_task(models, task, dims, **kwargs):\n",
    "    \"Validates `models` on task for `dims`.\"\n",
    "    if not hasattr(models, '__iter__'): models = [models]\n",
    "    if not hasattr(dims, '__iter__'): dims = [dims]\n",
    "    if len(models) != len(dims): \n",
    "        raise InputError(f\"There are {len(models)} models and {len(dims)} dimensions\")\n",
    "    pred_path = DATA_PATH/\"preds\"\n",
    "    pred_path.mkdir(exist_ok=True)\n",
    "    task_path = pred_path/f\"task{task}.txt\"\n",
    "    preds_dim = []\n",
    "    for model, dim in zip(models, dims): preds_dim.append(validate_model(model, task, dim=dim, **kwargs))\n",
    "    \n",
    "    with open(task_path, \"w\") as f:\n",
    "        for dim, preds in zip(dims, preds_dim):\n",
    "            for pred in preds:\n",
    "                f.write(f\"{int(dim)}; {';'.join(str(i.item()) for i in pred)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_data.ipynb.\n",
      "Converted 01_models.ipynb.\n",
      "Converted 02_prototypes.ipynb.\n",
      "Converted 03_utils.ipynb.\n",
      "Converted 04_analysis.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
