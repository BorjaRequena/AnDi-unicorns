{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from andi_unicorns.data import *\n",
    "from andi_unicorns.utils import *\n",
    "from andi_unicorns.models import *\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import mean_absolute_error as mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Example notebook\n",
    "> Example notebook showing how to load and predict with the models used in the AnDi Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The models are named following the convention `name_dim{dimension}_t{task}_{id}_custom.pth`. We've only had time to train the models for dimension 1 and tasks 1 and 2. The following function will load the ensemble asuming that the pre-trained models are in a `models/` directory, change the path at convenience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def load_task_model(task, dim=1, model_path=Path(\"models/\")):\n",
    "    \"Loads a pre-trained model given a task and a dimension.\"\n",
    "    if task == 1:   n_mod, act = 7, False\n",
    "    elif task == 2: n_mod, act = 10, True \n",
    "    names = [f\"hydra_dim{dim}_t{task}_{i}_custom.pth\" for i in range(n_mod)]\n",
    "    models = [load_model(name, path=model_path).cuda() for name in names]\n",
    "    for model in models: model.eval()\n",
    "    return Ensemble(models, add_act=act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The way our models work is with dataloaders that take the raw dataset in `.txt` format and transform it to a dataframe with pytorch tensors (may take a while). Provide a path to the directory where the `task{task}.txt` and `ref{task}.txt` files are. I am assuming you won't be trying to train a model, so the dataloader will be ready for validation, preserving the order of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def get_dataloader(task, path, dim=1, bs=128):\n",
    "    \"Provides dataloader from .txt files.\"\n",
    "    if not isinstance(path, Path): path = Path(path)\n",
    "    df = pd.DataFrame(columns=['dim', 'y', 'x', 'len'], dtype=object)\n",
    "    with open(path/f\"task{task}.txt\", \"r\") as D, open(path/f\"ref{task}.txt\") as Y:\n",
    "        trajs = csv.reader(D, delimiter=\";\", lineterminator=\"\\n\", quoting=csv.QUOTE_NONNUMERIC)\n",
    "        labels = csv.reader(Y, delimiter=\";\", lineterminator=\"\\n\", quoting=csv.QUOTE_NONNUMERIC)\n",
    "        for t, y in zip(trajs, labels):\n",
    "            d, x = int(t[0]), t[1:]\n",
    "            x = tensor(x).view(d, -1).T\n",
    "            label = tensor(y[1:]) if task is 3 else y[1]\n",
    "            df = df.append({'dim': d, 'y': label, 'x': x, 'len': len(x)}, ignore_index=True)\n",
    "    \n",
    "    df = df[df['dim'] == dim]\n",
    "    ds = L(zip(df['x'], df['y'])) if task == 1 else L(zip(df['x'], df['y'].astype(int)))\n",
    "    return DataLoader(ds, bs=bs, before_batch=pad_trajectories, device=default_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "In order to get the predictions, the next functions can be called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def get_preds_truth(model, dl): return get_preds(model, dl), get_truth(dl)\n",
    "\n",
    "def get_preds(model, dl):\n",
    "    \"Validates model on specific task and dimension.\"\n",
    "    return torch.cat([to_detach(model(xb)) for xb, _ in dl]) \n",
    "\n",
    "def get_truth(dl):\n",
    "    \"Retrieves labels from dataloader\"\n",
    "    return torch.cat([to_detach(yb) for _, yb in dl])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Task 1 example\n",
    "\n",
    "Here we assume that there's a directory `data/train` containing the validation data. Change the `data_path` at your convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "task = 1\n",
    "data_path = Path(\"data/train\")\n",
    "model = load_task_model(task)\n",
    "dl = get_dataloader(task, data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The predictions are the exponents so we can compute the mean absolute error straight away. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "preds, true = get_preds_truth(model, dl)\n",
    "score = mae(preds, true)\n",
    "print(f\"MAE: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Task 2 example\n",
    "\n",
    "Same as in the previous example, change the `data_path` at convenience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "task = 2\n",
    "data_path = Path(\"data/train\")\n",
    "model = load_task_model(task)\n",
    "dl = get_dataloader(task, data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "In this case, the predictions are in the format required for the submission. Hence, if we want to get the actual labels we need to call `.argmax(1)` over the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "preds, true = get_preds_truth(model, dl)\n",
    "labels = preds.argmax(1).squeeze()\n",
    "score = f1_score(true, labels, average='micro')\n",
    "print(f\"F1: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
