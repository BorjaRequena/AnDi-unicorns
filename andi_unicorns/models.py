# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01_models.ipynb (unless otherwise specified).

__all__ = ['MeanPredict', 'SimpleLSTM', 'ConcatPoolLSTM', 'mask_concat_pool', 'RegLSTM', 'PoolingClassifier']

# Cell
from fastai.text.all import *

# Cell
class MeanPredict:
    def __init__(self):
        self.mean = 0

    def fit(self, y): self.mean = y.mean()
    def predict(self, x): return torch.ones(x.shape[0]) * self.mean

# Cell
class SimpleLSTM(Module):
    "Cheap and simple LSTM running through the trajectories."
    def __init__(self, dim, h_size, vocab_sz, bs, n_layers=1, yrange=(0, 2.05)):
        self.rnn = nn.LSTM(dim, h_size, n_layers, batch_first=True)
        self.h_o = nn.Linear(h_size, vocab_sz)
        self.h = [torch.zeros(n_layers, bs, h_size) for _ in range(2)] # In case we do a generative
        self.sigmoid = SigmoidRange(*yrange)

    def forward(self, x):
        res, h = self.rnn(x)
        self.h = [h_.detach() for h_ in h]
        avg_pool = res.mean(1)   # Poorly done avg pooling
        out = self.h_o(avg_pool)
        return self.sigmoid(out).squeeze()

    def reset(self):
        for h in self.h: h.zero_()

# Cell
class ConcatPoolLSTM(Module):
    "LSTM with last, avg & max pooling."
    def __init__(self, dim, h_size, vocab_sz, n_layers=1, bidir=False, yrange=(0, 2.05), pad_value=0):
        self.pad_value = pad_value
        self.rnn = nn.LSTM(dim, h_size, n_layers, batch_first=True, bidirectional=bidir)
        self.h_o = nn.Linear(3*h_size, vocab_sz)
        self.sigmoid = SigmoidRange(*yrange)

    def forward(self, x):
        res, h = self.rnn(x)
        for h_ in h: h_.detach()
        mask = x == self.pad_value
        pool = mask_concat_pool(res, mask)
        out = self.h_o(pool)
        return self.sigmoid(out).squeeze()

def mask_concat_pool(output, mask):
    "Pool output of RNN with padding mask into one tensor [last_pool, avg_pool, max_pool]"
    lens = output.shape[1] - mask.sum(dim=1) + 1
    out_with_0 = output.masked_fill(mask, 0)
    out_with_inf = output.masked_fill(mask, -float('inf'))
    avg_pool = out_with_0.sum(dim=1).div_(lens.type(out_with_0.type()))
    max_pool = out_with_inf.max(dim=1)[0]
    return torch.cat([output[:, -1], avg_pool, max_pool], 1)

# Cell
class RegLSTM(Module):
    "LSTM with dropout and batch norm."
    def __init__(self, dim, h_size, vocab_sz=1, rnn_layers=1, linear_layers=[200, 50], ps=None, out_ps=0.2, y_range=(0, 2.05), pad_value=0):
        config = awd_lstm_clas_config
        self.pad_value = pad_value
        self.rnn = nn.LSTM(dim, h_size, rnn_layers, batch_first=True)

        lin_dim = [h_size*3] + linear_layers + [vocab_sz]
        if ps is None: ps = [0.1]*len(linear_layers)
        ps = [out_ps] + ps
        self.linear = PoolingClassifier(lin_dim, ps=ps, y_range=y_range)

    def forward(self, x):
        out, h = self.rnn(x)
        for h_ in h: h_.detach()
        mask = x == self.pad_value
        x, _, _ = self.linear((out, mask))
        return x

class PoolingClassifier(Module):
    "Pooling linear classifier inspired by `PoolingLinearClassifier`"
    def __init__(self, dims, ps, y_range=None):
        if len(ps) != len(dims)-1: raise ValueError(f"Number of layers {len(dims)} and dropout values {len(ps)} don't match.")
        acts = [nn.ReLU(inplace=True)] * (len(dims) - 2) + [None]
        layers = [LinBnDrop(i, o, p=p, act=a) for i,o,p,a in zip(dims[:-1], dims[1:], ps, acts)]
        if y_range is not None: layers.append(SigmoidRange(*y_range))
        self.layers = nn.Sequential(*layers)

    def forward(self, x):
        out, mask = x
        x = mask_concat_pool(out, mask)
        x = self.layers(x)
        return x, out, out