# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02_prototypes.ipynb (unless otherwise specified).

__all__ = ['RNNCNNClassifier']

# Cell
class RNNCNNClassifier(Module):
    def __init__(self, dim, vocab_sz=1, rnn_h=750, rnn_layers=1, in_p=0.4, rnn_p=0.3, w_p=0.5, out_p=0.4, bidir=False,
                 cnn_h=512, cnn_exp=1, cnn_layers=[1, 1, 1, 1], cnn_stemsz=(32, 64), cnn_ks=3, cnn_p=0.5, linear_layers=[200, 50], ps=None,
                 act=True, yrange=(0, 2.05), pad_value=0):
        store_attr('pad_value')
        self.rnn = RegLSTMLin(dim, rnn_h, vocab_sz=3*rnn_h, rnn_layers=rnn_layers, in_p=in_p, hid_p=rnn_p, weight_p=w_p, out_ps=out_p,
                              linear_layers=[], bidir=bidir, act=False)
        self.cnn = CNN(dim, exp=cnn_exp, layers=cnn_layers, p=cnn_p, n_out=cnn_h, stem_szs=cnn_stemsz)

        lin_dim = [3*rnn_h + cnn_h] + linear_layers + [vocab_sz]
        if ps is None: ps = [0.2]*len(linear_layers)
        ps = [out_p] + ps
        self.linear = Classifier(lin_dim, ps=ps, act=act, yrange=yrange)
        self.blocks = [self.rnn, self.cnn, self.linear]

    def forward(self, x):
        mask = x == self.pad_value
        x = mask_normalisation(x, mask)
        out_rnn, out_cnn = self.rnn(x), self.cnn(x.permute(0, 2, 1))
        x = torch.cat([out_rnn[0].relu_(), out_cnn.relu_()], dim=1)
        x = self.linear(x)
        return x, out_rnn[1], out_rnn[2]